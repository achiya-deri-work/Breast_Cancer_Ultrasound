{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torchvision.transforms import v2\n",
    "import winsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Functions import one_hot, accuracy, randint_distinct, augmant_data\n",
    "from models import BaseCNNClassifier, ResNet18, ResNet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "torch.random.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to CUDA if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a composition of transformations for data augmentation\n",
    "transforms = v2.Compose([\n",
    "    # Convert image to PIL Image format\n",
    "    v2.ToPILImage(),\n",
    "    # Randomly flip the image horizontally with 50% probability\n",
    "    v2.RandomHorizontalFlip(0.5),\n",
    "    # Randomly rotate the image by up to 45 degrees\n",
    "    v2.RandomRotation(45),\n",
    "    # Convert image to tensor format\n",
    "    v2.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for file names and image size\n",
    "files_names = ['benign', 'malignant', 'normal']\n",
    "image_sz = 224\n",
    "\n",
    "# Initialize empty tensors for X and y\n",
    "X = torch.tensor([], dtype=torch.float32)\n",
    "y = torch.tensor([], dtype=torch.int16)\n",
    "\n",
    "def collect_training_data(X, y):\n",
    "    \"\"\"\n",
    "    Collect training data from files in the current directory.\n",
    "\n",
    "    Args:\n",
    "        X (torch.tensor): Empty tensor to store image data\n",
    "        y (torch.tensor): Empty tensor to store class labels\n",
    "\n",
    "    Returns:\n",
    "        X (torch.tensor): Tensor with image data\n",
    "        y (torch.tensor): Tensor with class labels\n",
    "    \"\"\"\n",
    "    for file in files_names:\n",
    "        # Construct path to file and get class number\n",
    "        path = os.path.join(os.getcwd().replace('\\\\', '/'), file)\n",
    "        class_num = files_names.index(file)\n",
    "        print(f\"Class: {file} | Class value: {class_num}\")\n",
    "\n",
    "        # Iterate over images in the file\n",
    "        for img in tqdm(os.listdir(path)):\n",
    "            # Skip images with \"mask\" in the name\n",
    "            if \"mask\" not in img:\n",
    "                # Read image and resize to image_sz\n",
    "                img_array = cv.imread(os.path.join(path, img), cv.IMREAD_GRAYSCALE)\n",
    "                new_array = cv.resize(img_array, (image_sz, image_sz))\n",
    "\n",
    "                # Append image data to X and class label to y\n",
    "                X = torch.cat((X, torch.tensor(new_array, dtype=torch.float32).unsqueeze(0)), dim=0)\n",
    "                y = torch.cat((y, torch.tensor([class_num], dtype=torch.int16).unsqueeze(0)), dim=0)\n",
    "\n",
    "    # Normalize image data and add channel dimension\n",
    "    X = X / 256\n",
    "    X = X.unsqueeze(1)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Call the function to collect training data\n",
    "X, y = collect_training_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = X_train.to(device), X_test.to(device), y_train.to(device), y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment the data\n",
    "X_temp, y_temp = augmant_data(X_train, y_train, transforms, 3, True)\n",
    "X_train, y_train = torch.cat((X_temp, X_train), dim=0), torch.cat((y_temp, y_train), dim=0)\n",
    "del X_temp, y_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CNN model\n",
    "model_0 = BaseCNNClassifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameters\n",
    "batch_size = 32\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 5000\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer_0 = torch.optim.AdamW(model_0.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Get random batches of data and one hot encode the labels\n",
    "    batch_idxs = randint_distinct(0, X_train.size(0), batch_size)\n",
    "    X_batch, y_batch = X_train[batch_idxs], y_train[batch_idxs]\n",
    "    y_batch = one_hot(y_batch).to(device)\n",
    "\n",
    "    # Forward pass, compute loss, compute accuracy\n",
    "    y_pred = model_0(X_batch)\n",
    "    loss = loss_fn(y_pred, y_batch)\n",
    "    train_acc = accuracy(y_pred, y_batch)\n",
    "\n",
    "    # Compute backpropagation and optimization\n",
    "    optimizer_0.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_0.step()\n",
    "\n",
    "    # Evaluate the model on a test set every 10 epochs\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        model_0.eval()\n",
    "        test_loss = 0\n",
    "        test_acc = 0\n",
    "        with torch.inference_mode():\n",
    "            for i in range(6):\n",
    "                # Get random batches of data and one hot encode the labels\n",
    "                X_batch, y_batch = X_test[X_test.size(0) // 6 * i: X_test.size(0) // 6 * (i + 1)], y_test[X_test.size(0) // 6 * i: X_test.size(0) // 6 * (i + 1)]\n",
    "                y_batch = one_hot(y_batch).to(device)\n",
    "\n",
    "                # Forward pass, compute loss, compute accuracy\n",
    "                test_logits = model_0(X_batch)\n",
    "                test_loss += loss_fn(test_logits, y_batch).item()\n",
    "                test_acc += accuracy(test_logits, y_batch).item()\n",
    "        test_loss /= 6\n",
    "        test_acc /= 6\n",
    "        model_0.train()\n",
    "\n",
    "        # Print the loss and accuracy\n",
    "        print(f\"Epoch: {epoch+1} | Train loss: {loss.item():.4f} | Train Acc: {train_acc.item():.4f} | Test loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\\n\")\n",
    "\n",
    "        # Save the model if the test accuracy is higher than 0.8 and higher than the previous best accuracy\n",
    "        if test_acc > 0.8 and test_acc > model_0.best_acc:\n",
    "            # Get current model accuracy and index\n",
    "            model_0.best_acc = test_acc\n",
    "            idxs = [int(file.split(\"Idx\")[1][:-4]) for file in os.listdir(os.getcwd().replace('\\\\', '/') + \"/saves\") if \"Idx\" in file]\n",
    "            idx = max(idxs) + 1 if idxs else 0\n",
    "            \n",
    "            # Save the model\n",
    "            torch.save(model_0.state_dict(), os.getcwd().replace('\\\\', '/') + f\"/saves/BaseCNNAcc{test_acc:.5f}Idx{idx}.pth\")\n",
    "            print(f\"Model saved successfully with accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Print the best accuracy\n",
    "print(f\"Best Accuracy: {model_0.best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ResNet18 model\n",
    "model_1 = ResNet18().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameters\n",
    "batch_size = 32\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 5000\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model_1.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Get random batches of data and one hot encode the labels\n",
    "    batch_idxs = randint_distinct(0, X_train.size(0), batch_size)\n",
    "    X_batch, y_batch = X_train[batch_idxs], y_train[batch_idxs]\n",
    "    y_batch = one_hot(y_batch).to(device)\n",
    "\n",
    "    # Forward pass, compute loss, compute accuracy\n",
    "    y_pred = model_1(X_batch)\n",
    "    loss = loss_fn(y_pred, y_batch)\n",
    "    train_acc = accuracy(y_pred, y_batch)\n",
    "\n",
    "    # Compute backpropagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate the model on a test set every 10 epochs\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        model_1.eval()\n",
    "        test_loss = 0\n",
    "        test_acc = 0\n",
    "        with torch.inference_mode():\n",
    "            for i in range(6):\n",
    "                # Get random batches of data and one hot encode the labels\n",
    "                X_batch, y_batch = X_test[X_test.size(0) // 6 * i: X_test.size(0) // 6 * (i + 1)], y_test[X_test.size(0) // 6 * i: X_test.size(0) // 6 * (i + 1)]\n",
    "                y_batch = one_hot(y_batch).to(device)\n",
    "\n",
    "                # Forward pass, compute loss, compute accuracy\n",
    "                test_logits = model_1(X_batch)\n",
    "                test_loss += loss_fn(test_logits, y_batch).item()\n",
    "                test_acc += accuracy(test_logits, y_batch).item()\n",
    "        test_loss /= 6\n",
    "        test_acc /= 6\n",
    "        model_1.train()\n",
    "\n",
    "        # Print the loss and accuracy\n",
    "        print(f\"Epoch: {epoch+1} | Train loss: {loss.item():.4f} | Train Acc: {train_acc.item():.4f} | Test loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\\n\")\n",
    "\n",
    "        # Save the model if the test accuracy is higher than 0.8 and higher than the previous best accuracy\n",
    "        if test_acc > 0.8 and test_acc > model_1.best_acc:\n",
    "            # Get current model accuracy and index\n",
    "            model_1.best_acc = test_acc\n",
    "            idxs = [int(file.split(\"Idx\")[1][:-4]) for file in os.listdir(os.getcwd().replace('\\\\', '/') + \"/saves\") if \"Idx\" in file]\n",
    "            idx = max(idxs) + 1 if idxs else 0\n",
    "            \n",
    "            # Save the model\n",
    "            torch.save(model_1.state_dict(), os.getcwd().replace('\\\\', '/') + f\"/saves/ResNet18Acc{test_acc:.5f}Idx{idx}.pth\")\n",
    "            print(f\"Model saved successfully with accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Print the best accuracy\n",
    "print(f\"Best accuracy: {model_1.best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ResNet34 model\n",
    "model_2 = ResNet34().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameters\n",
    "batch_size = 32\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 5000\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model_2.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Get random batches of data and one hot encode the labels\n",
    "    batch_idxs = randint_distinct(0, X_train.size(0), batch_size)\n",
    "    X_batch, y_batch = X_train[batch_idxs], y_train[batch_idxs]\n",
    "    y_batch = one_hot(y_batch).to(device)\n",
    "\n",
    "    # Forward pass, compute loss, compute accuracy\n",
    "    y_pred = model_2(X_batch)\n",
    "    loss = loss_fn(y_pred, y_batch)\n",
    "    train_acc = accuracy(y_pred, y_batch)\n",
    "\n",
    "    # Compute backpropagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate the model on a test set every 10 epochs\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        model_2.eval()\n",
    "        test_loss = 0\n",
    "        test_acc = 0\n",
    "        with torch.inference_mode():\n",
    "            for i in range(6):\n",
    "                # Get random batches of data and one hot encode the labels\n",
    "                X_batch, y_batch = X_test[X_test.size(0) // 6 * i: X_test.size(0) // 6 * (i + 1)], y_test[X_test.size(0) // 6 * i: X_test.size(0) // 6 * (i + 1)]\n",
    "                y_batch = one_hot(y_batch).to(device)\n",
    "\n",
    "                # Forward pass, compute loss, compute accuracy\n",
    "                test_logits = model_2(X_batch)\n",
    "                test_loss += loss_fn(test_logits, y_batch).item()\n",
    "                test_acc += accuracy(test_logits, y_batch).item()\n",
    "        test_loss /= 6\n",
    "        test_acc /= 6\n",
    "        model_2.train()\n",
    "\n",
    "        # Print the loss and accuracy\n",
    "        print(f\"Epoch: {epoch+1} | Train loss: {loss.item():.4f} | Train Acc: {train_acc.item():.4f} | Test loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\\n\")\n",
    "\n",
    "        # Save the model if the test accuracy is higher than 0.8 and higher than the previous best accuracy\n",
    "        if test_acc > 0.8 and test_acc > model_2.best_acc:\n",
    "            # Get current model accuracy and index\n",
    "            model_2.best_acc = test_acc\n",
    "            idxs = [int(file.split(\"Idx\")[1][:-4]) for file in os.listdir(os.getcwd().replace('\\\\', '/') + \"/saves\") if \"Idx\" in file]\n",
    "            idx = max(idxs) + 1 if idxs else 0\n",
    "\n",
    "            # Save the model\n",
    "            torch.save(model_2.state_dict(), os.getcwd().replace('\\\\', '/') + f\"/saves/ResNet34Acc{test_acc:.5f}Idx{idx}.pth\")\n",
    "            print(f\"Model saved successfully with accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Print the best accuracy\n",
    "print(f\"Best accuracy: {model_2.best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import torchvision models for pre-trained models\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pytorch's pretrained ResNet34 model\n",
    "model_3 = models.resnet34(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first conv layer takes RGB input and not GRAYSCALE as we have, so we will adjust the data by duplicating the channels.\n",
    "model_3.conv1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreating the dataset as a too big augmented dataset proved to be less effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = X_train.to(device), X_test.to(device), y_train.to(device), y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment the data\n",
    "X_temp, y_temp = augmant_data(X_train, y_train, transforms, 2, True)\n",
    "X_train, y_train = torch.cat((X_temp, X_train), dim=0), torch.cat((y_temp, y_train), dim=0)\n",
    "del X_temp, y_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data to (3, 224, 224) as pretrained model expects this shape\n",
    "X_train, X_test = X_train.repeat(1, 3, 1, 1), X_test.repeat(1, 3, 1, 1)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the output layer from 1000 to 3 classes and add dropout\n",
    "model_3.fc = nn.Sequential(\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(in_features=512, out_features=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = model_3.to(device) # Moving the model to GPU\n",
    "model_3.best_acc = 0 # Initialize the best accuracy to 0\n",
    "model_3.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameters\n",
    "batch_size = 32\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 1500\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_3.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Get random batches of data and one hot encode the labels\n",
    "    batch_idxs = randint_distinct(0, X_train.size(0), batch_size)\n",
    "    X_batch, y_batch = X_train[batch_idxs], y_train[batch_idxs]\n",
    "    y_batch = one_hot(y_batch).to(device)\n",
    "\n",
    "    # Forward pass, compute loss, compute accuracy\n",
    "    y_pred = model_3(X_batch)\n",
    "    loss = loss_fn(y_pred, y_batch)\n",
    "    train_acc = accuracy(y_pred, y_batch)\n",
    "\n",
    "    # Compute backpropagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate the model on a test set every 10 epochs\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        model_3.eval()\n",
    "        test_loss = 0\n",
    "        test_acc = 0\n",
    "        with torch.inference_mode():\n",
    "            for i in range(6):\n",
    "                # Get random batches of data and one hot encode the labels\n",
    "                X_batch, y_batch = X_test[X_test.size(0) // 6 * i: X_test.size(0) // 6 * (i + 1)], y_test[X_test.size(0) // 6 * i: X_test.size(0) // 6 * (i + 1)]\n",
    "                y_batch = one_hot(y_batch).to(device)\n",
    "\n",
    "                # Forward pass, compute loss, compute accuracy\n",
    "                test_logits = model_3(X_batch)\n",
    "                test_loss += loss_fn(test_logits, y_batch).item()\n",
    "                test_acc += accuracy(test_logits, y_batch).item()\n",
    "        test_loss /= 6\n",
    "        test_acc /= 6\n",
    "        model_3.train()\n",
    "\n",
    "        # Print the loss and accuracy\n",
    "        print(f\"Epoch: {epoch+1} | Train loss: {loss.item():.4f} | Train Acc: {train_acc.item():.4f} | Test loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\\n\")\n",
    "\n",
    "        # Save the model if the test accuracy is higher than 0.8 and higher than the previous best accuracy\n",
    "        if test_acc > 0.9 and test_acc > model_3.best_acc:\n",
    "            # Get current model accuracy and index\n",
    "            model_3.best_acc = test_acc\n",
    "            idxs = [int(file.split(\"Idx\")[1][:-4]) for file in os.listdir(os.getcwd().replace('\\\\', '/') + \"/saves\") if \"Idx\" in file]\n",
    "            idx = max(idxs) + 1 if idxs else 0\n",
    "\n",
    "            # Save the model\n",
    "            torch.save(model_3.state_dict(), os.getcwd().replace('\\\\', '/') + f\"/saves/ResNet34Acc{test_acc:.5f}Idx{idx}.pth\")\n",
    "            print(f\"Model saved successfully with accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Print the best accuracy\n",
    "print(f\"Best accuracy: {model_3.best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pytorch's pretrained ResNet50 model\n",
    "model_4 = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the output layer from 1000 to 3 classes and add dropout\n",
    "model_4.fc = nn.Sequential(\n",
    "    nn.Dropout(0.6),\n",
    "    nn.Linear(in_features=2048, out_features=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = model_4.to(device) # Moving the model to GPU\n",
    "model_4.best_acc = 0 # Initialize the best accuracy to 0\n",
    "model_4.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameters\n",
    "batch_size = 32\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 1500\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_4.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Train loss: 0.0072 | Train Acc: 1.0000 | Test loss: 0.5062 | Test Acc: 0.8718\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m batch_idxs \u001b[38;5;241m=\u001b[39m randint_distinct(\u001b[38;5;241m0\u001b[39m, X_train\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), batch_size)\n\u001b[0;32m      5\u001b[0m X_batch, y_batch \u001b[38;5;241m=\u001b[39m X_train[batch_idxs], y_train[batch_idxs]\n\u001b[1;32m----> 6\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m \u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Forward pass, compute loss, compute accuracy\u001b[39;00m\n\u001b[0;32m      9\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model_4(X_batch)\n",
      "File \u001b[1;32mc:\\Users\\achiy\\vs_code_main\\python_data\\breast_cancer_ultrasound_imgs\\Functions.py:16\u001b[0m, in \u001b[0;36mone_hot\u001b[1;34m(y, num_classes)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm \u001b[38;5;28;01mas\u001b[39;00m TDQM\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03mContains various functions for data processing.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m    augmant_data: Data augmentation for the input data given 'transforms'.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_hot\u001b[39m(y, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m     17\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    Generates a one-hot encoded tensor from the given tensor `y`.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m    between 0 and `num_classes - 1`.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# Get the shape of the input tensor\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Get random batches of data and one hot encode the labels\n",
    "    batch_idxs = randint_distinct(0, X_train.size(0), batch_size)\n",
    "    X_batch, y_batch = X_train[batch_idxs], y_train[batch_idxs]\n",
    "    y_batch = one_hot(y_batch).to(device)\n",
    "\n",
    "    # Forward pass, compute loss, compute accuracy\n",
    "    y_pred = model_4(X_batch)\n",
    "    loss = loss_fn(y_pred, y_batch)\n",
    "    train_acc = accuracy(y_pred, y_batch)\n",
    "\n",
    "    # Compute backpropagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate the model on a test set every 10 epochs\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        model_4.eval()\n",
    "        test_loss = 0\n",
    "        test_acc = 0\n",
    "        with torch.inference_mode():\n",
    "            for i in range(6):\n",
    "                # Get random batches of data and one hot encode the labels\n",
    "                X_batch, y_batch = X_test[X_test.size(0) // 6 * i: X_test.size(0) // 6 * (i + 1)], y_test[X_test.size(0) // 6 * i: X_test.size(0) // 6 * (i + 1)]\n",
    "                y_batch = one_hot(y_batch).to(device)\n",
    "\n",
    "                # Forward pass, compute loss, compute accuracy\n",
    "                test_logits = model_4(X_batch)\n",
    "                test_loss += loss_fn(test_logits, y_batch).item()\n",
    "                test_acc += accuracy(test_logits, y_batch).item()\n",
    "        test_loss /= 6\n",
    "        test_acc /= 6\n",
    "        model_4.train()\n",
    "\n",
    "        # Print the loss and accuracy\n",
    "        print(f\"Epoch: {epoch+1} | Train loss: {loss.item():.4f} | Train Acc: {train_acc.item():.4f} | Test loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\\n\")\n",
    "\n",
    "        # Save the model if the test accuracy is higher than 0.8 and higher than the previous best accuracy\n",
    "        if test_acc > 0.92 and test_acc > model_4.best_acc:\n",
    "            # Get current model accuracy and index\n",
    "            model_4.best_acc = test_acc\n",
    "            idxs = [int(file.split(\"Idx\")[1][:-4]) for file in os.listdir(os.getcwd().replace('\\\\', '/') + \"/saves\") if \"Idx\" in file]\n",
    "            idx = max(idxs) + 1 if idxs else 0\n",
    "\n",
    "            # Save the model\n",
    "            torch.save(model_4.state_dict(), os.getcwd().replace('\\\\', '/') + f\"/saves/ResNet50Acc{test_acc:.5f}Idx{idx}.pth\")\n",
    "            print(f\"Model saved successfully with accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Print the best accuracy\n",
    "print(f\"Best accuracy: {model_4.best_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
